{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:1.5em\">\n",
    "    <p>üìú Table des mati√®res:</p>\n",
    "    <ul>\n",
    "       <li><a href=\"#nltk\">Text Preprocessing with NLTK</a></li>  \n",
    "       <li>\n",
    "          <a href=\"#spacy\">Text Preprocessing with spaCy</a>\n",
    "       </li>\n",
    "       <li>\n",
    "          <a href=\"#comp\">Comparing results after spaCy's NER</a>\n",
    "       </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id =\"nltk\">Text Preprocessing with NLTK</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gisevciconia zero solution objet ordre virement Monsieur d√©bit compte :_rib: 022 222 222 172 22 29576475 53 set agilisys zero solution swift agilamcxxx virer montant 89 250,00 dirais (quatre-vingt-neuf mille deux cent cinquante dirais de profit i parler export switzerland ian chez 0483 555 555 0266600 0 swift creschzz80a motif 22a005 signature agilisys industrie sara siege social angle ne rue ibn aisha boulevard abdelkrim el khattabi millim√®tre pari 2TM* etage app√¢t ne gueulez marrakech tu +212 (0) 999 99 6161 f +212 (0) 999 999 130 sara capital 500.000 000 des rcn35 33 if n 3939333 ton 3939 cnss:939 mt-103-stp 2838383393 et point pote trot cc cs con instance type and transmission de original receive from\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer , TreebankWordTokenizer,WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "import unicodedata\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "def text_preprocessing(text,lang,accented=True,tokenizer=WhitespaceTokenizer(),stopw=True,punctuation=True,lowercase=True,lemmatize=True,spelling=True,expand_contraction=True,urls=True):\n",
    "    if lang.lower()=='english':\n",
    "        stopword =stopwords.words('english')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        spell = SpellChecker()\n",
    "    else :#if lang=\"french\"\n",
    "        stopword = stopwords.words('french')\n",
    "        lemmatizer = FrenchLefffLemmatizer()\n",
    "        spell = SpellChecker(language=\"fr\")\n",
    "    if lowercase:\n",
    "        #lowercase the text \n",
    "        text = text.lower()\n",
    "    if urls:\n",
    "        #remove urls\n",
    "        text=remove_urls(text)\n",
    "    #tokenize the text \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if expand_contraction:\n",
    "        #expand contractions\n",
    "        tokens = [contractions.fix(token) for token in tokens]\n",
    "    if punctuation:\n",
    "        #remove punctuation\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    if stopw:\n",
    "        #remove stopwords\n",
    "        tokens = [token for token in tokens if token not in stopword]\n",
    "    if accented:\n",
    "        tokens = [remove_accented_chars(token) for token in tokens]\n",
    "    if spelling:\n",
    "        #spell check:\n",
    "        tokens = [spell.correction(token) for token in tokens]\n",
    "    if lemmatize:\n",
    "        #lemmatization : \n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(word for word in tokens)\n",
    "\n",
    "    #Some tests:\n",
    "file = 'C:/Users/PC2/Downloads/test-ex.txt'\n",
    "f=open(file,'r')\n",
    "data = f.read()\n",
    "print(text_preprocessing(data,lang='french'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id =\"spacy\">Text Preprocessing with spaCy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gisevciconia  aero solutions  objet  ordre virement  monsieur   d√©bit compte  _ rib  022 222 222 172 22 29576475 53  swt  agilisy aero solution  swift  agilamcxx   virer montant  89 25000 dirham  quatrevingtneuf dirham     profit  carlex export switzerland  iban  ch35 0483 555 555 0266600 0  swift  creschzz80a  motif  22a005  signature   agilisys industrie sarl  si√©g social  angle num√©ro 2 rue ibn aicha bd abdelkrim el khattabi imm pari 2 degr√©   √©tage appt num√©ro 23 gueliz  marrakech  t  212  0  999 99 6161 l f  212  0  999 999 130   sarl capital 500000 000 dhs  rcn degr√© 35 33  if num√©ro 3939333  tpn degr√© 3939  cnss939  mt103stp 2838383393  mt print   poet trot ccc cs econ instance type and transmission   original received from \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Now we ll do preprocessing using mainly spacy\n",
    "import spacy\n",
    "#load only french and english models tokenizers\n",
    "nlp_en = spacy.load(\"en_core_web_sm\", disable=['parser', 'tagger', 'ner'])\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\", disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def spacy_preprocessing(text,lang,lowercase=True,stopw=True,punctuation=True,alphabetic=True,lemmatize=True,):\n",
    "    if lang ==\"en\":\n",
    "        nlp = nlp_en\n",
    "    else :\n",
    "        nlp = nlp_fr\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    #tokenize with spacy's default tokenizer\n",
    "    tokens = nlp(text)\n",
    "    if stopw :\n",
    "        tokens = [token for token in tokens if not token.is_stop]\n",
    "    if lemmatize :\n",
    "        tokens = [token.lemma_.strip() for token in tokens]\n",
    "    if punctuation :\n",
    "        tokens = [re.sub('<[^>]*>', '', token) for token in tokens]\n",
    "    if alphabetic:\n",
    "        tokens = [re.sub('[\\W]+','',token.lower()) for token in tokens]\n",
    "    return ' '.join(word for word in tokens)\n",
    "\n",
    "file = 'C:/Users/PC2/Downloads/test-ex.txt'\n",
    "f=open(file,'r')\n",
    "data = f.read()\n",
    "print(spacy_preprocessing(data,lang='french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id =\"comp\">Comparing results after spaCy's NER</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messieurs   PER       Named person or family.\n",
      "AGILAMCXXX\n",
      "|ORG       Companies, agencies, institutions, etc.\n",
      "Montant     LOC       Non-GPE locations, mountain ranges, bodies of water\n",
      "Dirhams     MISC      Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "Mille deux cent cinquanteMISC      Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "Dirhams     MISC      Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "CH35        LOC       Non-GPE locations, mountain ranges, bodies of water\n",
      "SWIFT       ORG       Companies, agencies, institutions, etc.\n",
      "CRESCHZZ80A\n",
      "\n",
      " \n",
      "\n",
      "MotifMISC      Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "INDUSTRIES SARLMISC      Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "Angle n¬∞2 rue ibn aichaMISC      Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "Abdelkrim el khattabiLOC       Non-GPE locations, mountain ranges, bodies of water\n",
      "Paris       LOC       Non-GPE locations, mountain ranges, bodies of water\n",
      "Gueliz      LOC       Non-GPE locations, mountain ranges, bodies of water\n",
      "Marrakech\n",
      "T PER       Named person or family.\n",
      "F           LOC       Non-GPE locations, mountain ranges, bodies of water\n",
      "Capital     LOC       Non-GPE locations, mountain ranges, bodies of water\n",
      "dhs | RCN   MISC      Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "MT Print    PER       Named person or family.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "file = 'C:/Users/PC2/Downloads/test-ex.txt'\n",
    "f=open(file,'r')\n",
    "data = f.read()\n",
    "doc = nlp(data)\n",
    "for ent in doc.ents :\n",
    "    print('{:<12}{:<10}{:<10}'.format(ent.text,ent.label_,spacy.explain(str(ent.label_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gisevciconia zero:PER            :Named person or family.\n",
      "Monsieur       :PER            :Named person or family.\n",
      "zero solution swift:MISC           :Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "switzerland ian:MISC           :Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "el khattabi    :ORG            :Companies, agencies, institutions, etc.\n",
      "app√¢t          :PER            :Named person or family.\n",
      "gueulez marrakech:LOC            :Non-GPE locations, mountain ranges, bodies of water\n",
      "rcn35          :LOC            :Non-GPE locations, mountain ranges, bodies of water\n",
      "cnss:939 mt-103-stp 2838383393:MISC           :Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "nltk_preprocessed = text_preprocessing(data,'fr')\n",
    "doc  = nlp(nltk_preprocessed)\n",
    "for ent in doc.ents :\n",
    "    print('{:<15}:{:<15}:{:<15}'.format(ent.text,ent.label_,spacy.explain(str(ent.label_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swt  agilisy aero solution  swift  agilamcxx   :MISC      :Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "dirham      :MISC      :Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "switzerland  iban  ch35:MISC      :Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "el khattabi imm:ORG       :Companies, agencies, institutions, etc.\n",
      "dhs         :ORG       :Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "spacy_preprocessed = spacy_preprocessing(data,'fr')\n",
    "doc  = nlp(spacy_preprocessed)\n",
    "for ent in doc.ents :\n",
    "    print('{:<12}:{:<10}:{:<10}'.format(ent.text,ent.label_,spacy.explain(str(ent.label_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77b840b16efc606bfe6fe465e16c80fde01359cf425f7f37ae28958c7f375ddc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
